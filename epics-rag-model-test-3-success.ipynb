{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bb9a28c7-abc5-4100-b0af-64bdcccc1301",
    "_uuid": "71f4c7b6-d06f-429c-9826-561311122b66",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu langchain langchain_community sentence-transformers bitsandbytes langchain_huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "csv_path = \"Animal disease spreadsheet - Sheet1.csv\"\n",
    "\n",
    "loader = CSVLoader(file_path=csv_path,encoding='utf-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    \n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    split_docs,\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "vector_store = 'index.faiss'\n",
    "\n",
    "vector_store.save_local(r\"D:\\Projectss\\EPICS Chatbot\\FAISS\")\n",
    "\n",
    "# After creating the embeddings you can load the embedding by uncommenting the below line and commenting above lines\n",
    "# vector_store = FAISS.load_local(folder_path='FAISS',embeddings=embedding_model,allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc090dc56fe4e63b7fd1cd8e9f7e8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for cuda model\n",
    "model_name = \"tiiuae/Falcon3-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 4-bit configuration\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 8-bit configuration\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "### Modified Model Loading with Quantization Options\n",
    "def load_quantized_model(model_name, use_quantization='8bit'):\n",
    "    if use_quantization == '4bit':\n",
    "        quant_config = bnb_config_4bit\n",
    "    elif use_quantization == '8bit':\n",
    "        quant_config = bnb_config_8bit\n",
    "    else:\n",
    "        quant_config = None\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=\"auto\" if not quant_config else None\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = load_quantized_model(model_name,'none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    # temperature=0.7,\n",
    "    # top_p=0.9,\n",
    "    # repetition_penalty=1.1,\n",
    "    return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rag_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the question only if it is explicitly about veterinary diseases or animal health. Follow these steps:  \n",
    "1. Check Scope:\n",
    "   - If the question is unrelated to veterinary topics, say: \"I don't know. My expertise is limited to veterinary diseases and animal health.\"  \n",
    "   - Do not use the context for non-veterinary questions.\n",
    "2. Veterinary Answers:  \n",
    "   - If veterinary-related, answer directly and concisely using the provided context.  \n",
    "   - Do not add self-generated questions, hypothetical scenarios, or unrelated topics.\n",
    "   - Only supplement with veterinary knowledge if the context is insufficient. \\nContext: {context} \\nQuestion: {question} \\nAnswer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def ask(query):\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the RAG chain\n",
    "    result = rag_chain.invoke({\"query\": query})\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    \n",
    "    # Calculate tokens/second\n",
    "    # Get the generated text and tokenize it\n",
    "    generated_text = result[\"result\"]\n",
    "    tokens = tokenizer(generated_text, return_tensors=\"pt\").input_ids.shape[1]\n",
    "    tokens_per_second = tokens / time_taken\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Question:\", query)\n",
    "    print(\"\\nAnswer:\", result[\"result\"])\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(f\"- Time taken: {time_taken:.2f} seconds\")\n",
    "    print(f\"- Tokens generated: {tokens}\")\n",
    "    print(f\"- Tokens/second: {tokens_per_second:.2f}\")\n",
    "    \n",
    "    print(\"\\nSources:\")\n",
    "    for doc in result[\"source_documents\"][:2]:  # Show top 2 sources\n",
    "        print(f\"- {doc.page_content[:150]}...\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ask(\"Blue Tongue and its preventions\")\n",
    "ask(\"Give info about anthrax\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6547902,
     "sourceId": 10580469,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
